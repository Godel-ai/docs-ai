### Interpretable GAN Controls

- [**[2004.02546] GANSpace: Discovering Interpretable GAN Controls**](https://arxiv.org/abs/2004.02546)
- [**harskish/ganspace: Discovering Interpretable GAN Controls**](https://github.com/harskish/ganspace)
- [**Improved Techniques for Training Single-Image GANs**](https://paperswithcode.com/paper/improved-techniques-for-training-single-image)
- [**On the "steerability" of generative adversarial networks**](https://arxiv.org/abs/1907.07171.pdf)
- [**facebookresearch/GAN-optimization-landscape: code to reproduce the empirical results in the research paper**](https://github.com/facebookresearch/GAN-optimization-landscape)
- [**GANalyze: Toward Visual Definitions of Cognitive Image Properties**](https://arxiv.org/abs/1906.10112.pdf)
- [**Mimicry: Towards the Reproducibility of GAN Research**](https://paperswithcode.com/paper/mimicry-towards-the-reproducibility-of-gan-1)
- [**Token Manipulation Generative Adversarial Network for Text Generation**](https://paperswithcode.com/paper/token-manipulation-generative-adversarial)
- [**Interpreting_the_Latent_Space_of_GANs_for_Semantic_Face_Editing**](http://openaccess.thecvf.com/content_CVPR_2020/html/Shen_Interpreting_the_Latent_Space_of_GANs_for_Semantic_Face_Editing_CVPR_2020_paper.html)
- [**[1902.03984] Improving Generalization and Stability of Generative Adversarial Networks**](https://arxiv.org/abs/1902.03984)
- [**GAN Dissection: Visualizing and Understanding Generative Adversarial Networks**](https://arxiv.org/abs/1811.10597.pdf)
- [**[1912.00953] LOGAN: Latent Optimisation for Generative Adversarial Networks**](https://arxiv.org/abs/1912.00953)

### Training and Stability of Gans

- [**why wgan does not converge - Google Search**](https://www.google.com/search?q=why+wgan+does+not+converge&oq=why+wgan+does+not+&aqs=chrome.1.69i57j33.4870j0j7&sourceid=chrome&ie=UTF-8)
- [**Which Training Methods for GANs do actually Converge?**](https://arxiv.org/abs/1801.04406.pdf)
- [**Odena: Open questions about generative adversarial networks - Google Scholar**](https://scholar.google.com/scholar?cites=7681139046925332139&as_sdt=2005&sciodt=0,5&hl=en)
- [**Towards Principled Methods for Training Generative Adversarial Networks**](https://arxiv.org/abs/1701.04862.pdf)
- [**Improved Training of Wasserstein GANs**](https://arxiv.org/abs/1704.00028.pdf)
- [**VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning**](https://arxiv.org/abs/1705.07761.pdf)
- [**tamarott/SinGAN: Official pytorch implementation of the paper: "SinGAN: Learning a Generative Model from a Single Natural Image"**](https://github.com/tamarott/SinGAN)
- [**Input Convex Neural Networks**](https://arxiv.org/abs/1609.07152.pdf)
- [**Tunability: Importance of Hyperparameters of Machine Learning Algorithms**](http://www.jmlr.org/papers/volume20/18-444/18-444.pdf)
- [**[2004.13344] Robust Generative Adversarial Network**](https://arxiv.org/abs/2004.13344)
- [**Learning Convex Optimization Models**](https://arxiv.org/abs/2006.04248.pdf)
- [**AutoAugment: Learning Augmentation Strategies From Data**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.pdf)
- [**Automated Machine Learning Book**](https://library.oapen.org/bitstream/handle/20.500.12657/23012/1007149.pdf?sequence=1#page=15)
- [**Optimizing Millions of Hyperparameters by Implicit Differentiation**](http://proceedings.mlr.press/v108/lorraine20a/lorraine20a.pdf)
- [**Why is Developing Machine Learning Applications Challenging? A Study on Stack Overflow Posts**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8870187)
- [**Open Questions about Generative Adversarial Networks**](https://distill.pub/2019/gan-open-problems/)
- [**machine learning - GAN - am I seeing mode collapse? Common fixes not working - Data Science Stack Exchange**](https://datascience.stackexchange.com/questions/51276/gan-am-i-seeing-mode-collapse-common-fixes-not-working)
- [**GAN Lab: Play with Generative Adversarial Networks in Your Browser!**](https://poloclub.github.io/ganlab/)
- [**[1502.02127] Hyperparameter Search in Machine Learning**](https://arxiv.org/abs/1502.02127)
- [**Learning Disentangled Representations for Recommendation**](http://papers.nips.cc/paper/8808-learning-disentangled-representations-for-recommendation.pdf)
- [**learning rate scheduler for gans - Google Search**](https://www.google.com/search?q=learning+rate+scheduler+for+gans&oq=learning+rate+scheduler+for+gans&aqs=chrome..69i57j33.5185j0j7&sourceid=chrome&ie=UTF-8)
- [**Which Training Methods for GANs do actually Converge?**](https://arxiv.org/abs/1801.04406.pdf)
- [**Forget the Learning Rate, Decay Loss**](https://arxiv.org/abs/1905.00094.pdf)
- [**Stabilizing Training of Generative Adversarial Networks through Regularization**](https://papers.nips.cc/paper/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization.pdf)
- [**Roth: Stabilizing training of generative adversarial... - Google Scholar**](https://scholar.google.com/scholar?as_ylo=2019&hl=en&as_sdt=0,5&sciodt=0,5&cites=11974368860899381464&scipsc=)
- [**Multi-marginal Wasserstein GAN**](http://papers.nips.cc/paper/8454-multi-marginal-wasserstein-gan)
- [**A Large-Scale Study on Regularization and Normalization in GANs**](http://proceedings.mlr.press/v97/kurach19a.html)
- [**A Domain Agnostic Measure for Monitoring and Evaluating GANs**](http://papers.nips.cc/paper/9377-a-domain-agnostic-measure-for-monitoring-and-evaluating-gans)
- [**[2001.06937] A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications**](https://arxiv.org/abs/2001.06937)
- [**10 Lessons I Learned Training GANs for one Year - Towards Data Science**](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
- [**Gulrajani: Improved training of wasserstein gans - Google Scholar**](https://scholar.google.com/scholar?start=10&hl=en&as_sdt=0,5&sciodt=0,5&as_ylo=2020&cites=3068694056154618633&scipsc=)
- [**Improved Training of Wasserstein GANs**](https://arxiv.org/abs/1704.00028.pdf)
- [**[1802.05957] Spectral Normalization for Generative Adversarial Networks**](https://arxiv.org/abs/1802.05957)
- [**A Style-Based Generator Architecture for Generative Adversarial Networks**](http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html)
- [**Self-Attention Generative Adversarial Networks**](http://proceedings.mlr.press/v97/zhang19d.html)
  [**Small-GAN: Speeding Up GAN Training Using Core-sets**](https://arxiv.org/abs/1910.13540v1.pdf)
- [**common mistakes in training gans - Google Search**](https://www.google.com/search?safe=active&sxsrf=ACYBGNRdeNkjXLyKnFPprGxJDVUmhbAkSw%3A1568476766141&ei=Xg59XZOiCMXevgSpyoXICw&q=common+mistakes+in+training+gans&oq=common+mistakes+in+training+gans&gs_l=psy-ab.3..33i22i29i30.11244.15333..15478...3.2..0.317.2580.0j15j0j1......0....1..gws-wiz.......0i71j35i39j0i67j0j0i22i30j33i21j33i160.b4czx-_BCKg&ved=0ahUKEwiTye-j19DkAhVFr48KHSllAbkQ4dUDCAs&uact=5)
- [**Keep Calm and train a GAN. Pitfalls and Tips on training Generative Adversarial Networks**](https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9)
- [**Tips On Training Your GANs Faster and Achieve Better Results**](https://medium.com/intel-student-ambassadors/tips-on-training-your-gans-faster-and-achieve-better-results-9200354acaa5)
- [**How to Implement GAN Hacks in Keras to Train Stable Models**](https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/)
- [**Improved Techniques for Training GANs**](https://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)
- [**10 Lessons I Learned Training GANs for one Year - Towards Data Science**](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
- [**How to Implement GAN Hacks in Keras to Train Stable Models**](https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/)
- [**Tips for Training Stable Generative Adversarial Networks**](https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/)
- [**Generative Adversarial Networks with Python**](https://machinelearningmastery.com/generative_adversarial_networks/)

### GANs

- [**Modular Generative Adversarial Networks**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bo_Zhao_Modular_Generative_Adversarial_ECCV_2018_paper.pdf)
- [**Unpaired Photo-to-manga Translation Based on The Methodology of Manga Drawing**](https://arxiv.org/abs/2004.10634v1.pdf)
- [**Diverse Image Synthesis from Semantic Layouts via Conditional IMLE**](https://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/)
- [**3D Ken Burns Effect from a Single Image**](https://arxiv.org/abs/1909.05483v1.pdf)

- [**how to view gan output - Google Search**](https://www.google.com/search?q=how+to+view+gan+output&oq=how+to+view+gan+output&aqs=chrome..69i57.5406j0j7&sourceid=chrome&ie=UTF-8)
- [**A New Way to look at GANs - Towards Data Science**](https://towardsdatascience.com/a-new-way-to-look-at-gans-7c6b6e6e9737)
- [**Graduating in GANs: Going from understanding generative adversarial networks to running your own**](https://towardsdatascience.com/graduating-in-gans-going-from-understanding-generative-adversarial-networks-to-running-your-own-39804c283399)
- [**This Machine Learning Medium post does not exist - Comet.ml - Medium**](https://medium.com/comet-ml/this-machine-learning-medium-post-does-not-exist-c4705215b4a0)
- [**GAN Objective Functions: GANs and Their Variations - Towards Data Science**](https://towardsdatascience.com/gan-objective-functions-gans-and-their-variations-ad77340bce3c)
- [**The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection**](https://papers.nips.cc/paper/8315-the-point-where-reality-meets-fantasy-mixed-adversarial-generators-for-image-splice-detection.pdf)
- [**Large Scale GAN Training for High Fidelity Natural Image Synthesis**](https://arxiv.org/abs/1809.11096.pdf)
- [**A Learned Representation For Artistic Style**](https://arxiv.org/abs/1707.00683.pdf)
- [**[1905.12043] Video-to-Video Translation for Visual Speech Synthesis**](https://arxiv.org/abs/1905.12043)
- [**[1904.12356] Deferred Neural Rendering: Image Synthesis using Neural Textures**](https://arxiv.org/abs/1904.12356)
- [**[1905.02320] Spatially Constrained Generative Adversarial Networks for Conditional Image Generation**](https://arxiv.org/abs/1905.02320)
- [**Changing the Image Memorability: From Basic Photo Editing to GANs**](http://openaccess.thecvf.com/content_CVPRW_2019/html/MBCCV/Sidorov_Changing_the_Image_Memorability_From_Basic_Photo_Editing_to_GANs_CVPRW_2019_paper.html)
- [**[1909.07957] An Internal Learning Approach to Video Inpainting**](https://arxiv.org/abs/1909.07957)
- [**Neural Scene Decomposition for Multi-Person Motion Capture**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Rhodin_Neural_Scene_Decomposition_for_Multi-Person_Motion_Capture_CVPR_2019_paper.pdf)
- [**Vid2Game: Controllable Characters Extracted from Real-World Videos**](https://arxiv.org/abs/1904.08379.pdf)
- [**Optimal Unsupervised Domain Translation**](https://arxiv.org/abs/1906.01292.pdf)
- [**Unsupervised Learning of Object Structure and Dynamics from Videos**](https://arxiv.org/abs/1906.07889.pdf)
- [**Free-form Video Inpainting with 3D Gated Convolution and Temporal PatchGAN**](https://arxiv.org/abs/1904.10247.pdf)
- [**Deep Video Frame Interpolation using Cyclic Frame Generation**](https://www.citi.sinica.edu.tw/papers/yylin/6497-F.pdf)
- [**Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation**](https://arxiv.org/abs/1712.00080.pdf)
- [**Learning to See in the Dark**](https://arxiv.org/abs/1805.01934.pdf)
- [**Self-Attention Generative Adversarial Networks**](https://arxiv.org/abs/1805.08318.pdf)
- [**10.1145/3072959.3073640**](http://sci-hub.tw/https://dl.acm.org/citation.cfm?id=3073640 | Sci-Hub | Synthesizing Obama. ACM Transactions on Graphics, 36(4), 1–13 )
- [**State of the Art on 3D Reconstruction with RGB-D Cameras**](https://web.stanford.edu/~zollhoef/papers/EG18_RecoSTAR/paper.pdf)
- [**Face2Face: Real-time Face Capture and Reenactment of RGB Videos**](https://web.stanford.edu/~zollhoef/papers/CVPR2016_Face2Face/paper.pdf)
- [**facewarehouse-tr.pdf**](http://kunzhou.net/2012/facewarehouse-tr.pdf)
- [**iwanao731’s gists**](https://gist.github.com/iwanao731)
- [**ESanchezLozano/GANnotation: GANnotation (PyTorch): Landmark-guided face to face synthesis using GANs (And a triple consistency loss!)**](https://github.com/ESanchezLozano/GANnotation)
- [**ESanchezLozano/StarGAN-with-Triple-Consistency-Loss: StarGAN with a triple consistency loss**](https://github.com/ESanchezLozano/StarGAN-with-Triple-Consistency-Loss)
- [**Triple consistency loss for pairing distributions in GAN-based face synthesis**](https://arxiv.org/abs/1811.03492.pdf)
- [**datitran/face2face-demo: pix2pix demo that learns from facial landmarks and translates this into a face**](https://github.com/datitran/face2face-demo)
- [**Style Aggregated Network for Facial Landmark Detection**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf)
- [**Neural Face Editing With Intrinsic Image Disentangling**](http://openaccess.thecvf.com/content_cvpr_2017/html/Shu_Neural_Face_Editing_CVPR_2017_paper.html)
- [**Bringing portraits to life**](https://dl.acm.org/citation.cfm?id=3130818)
- [**Learning to Take Directions One Step at a Time**](https://arxiv.org/abs/1812.01874.pdf)
- [**Audio samples from "Uncovering Latent Style Factors for Expressive Speech Synthesis"**](https://google.github.io/tacotron/publications/uncovering_latent_style_factors_for_expressive_speech_synthesis/index.html)
- [**ESanchezLozano/GANnotation: GANnotation (PyTorch): Landmark-guided face to face synthesis using GANs (And a triple consistency loss!)**](https://github.com/ESanchezLozano/GANnotation)
- [**datitran/face2face-demo: pix2pix demo that learns from facial landmarks and translates this into a face**](https://github.com/datitran/face2face-demo)
- [**Face2face — A Pix2Pix demo that mimics the facial expression of the German chancellor**](https://towardsdatascience.com/face2face-a-pix2pix-demo-that-mimics-the-facial-expression-of-the-german-chancellor-b6771d65bf66)
- [**HeadOn: Real-time Reenactment of Human Portrait Videos**](https://arxiv.org/abs/1805.11729.pdf)
- [**Learning to Take Directions One Step at a Time**](https://arxiv.org/abs/1812.01874.pdf)
- [**Spectral Normalization for Generative Adversarial Networks**](https://arxiv.org/abs/1802.05957.pdf)
- [**What is the Effect of Importance Weighting in Deep Learning?**](http://proceedings.mlr.press/v97/byrd19a/byrd19a.pdf)
- [**How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks**](https://arxiv.org/abs/1808.04926.pdf)
- [**What is the Effect of Importance Weighting in Deep Learning?**](https://arxiv.org/abs/1803.04477.pdf)
- [**Generative Image Inpainting With Contextual Attention**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Generative_Image_Inpainting_CVPR_2018_paper.pdf)
- [**Neural Rerendering in the Wild**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Meshry_Neural_Rerendering_in_the_Wild_CVPR_2019_paper.pdf)
- [**Adversarial Inference for Multi-Sentence Video Description**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Adversarial_Inference_for_Multi-Sentence_Video_Description_CVPR_2019_paper.pdf)
- [**Mocycle-GAN: Unpaired Video-to-Video Translation**](https://arxiv.org/abs/1908.09514.pdf)
- [**Coordinate-Based Texture Inpainting for Pose-Guided Human Image Generation**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Grigorev_Coordinate-Based_Texture_Inpainting_for_Pose-Guided_Human_Image_Generation_CVPR_2019_paper.pdf)
- [**Neural Scene Decomposition for Multi-Person Motion Capture**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Rhodin_Neural_Scene_Decomposition_for_Multi-Person_Motion_Capture_CVPR_2019_paper.pdf)
- [**Neural Point-Based Graphics**](https://arxiv.org/abs/1906.08240.pdf)
- [**10.1145/3306346.3323006**](http://sci-hub.tw/https://dl.acm.org/citation.cfm?id=3323006 | Sci-Hub | Stylizing video by example. ACM Transactions on Graphics, 38(4), 1–11 )
- [**[1803.08887] Dist-GAN: An Improved GAN using Distance Constraints**](https://arxiv.org/abs/1803.08887)
- [**Depth-Aware Video Frame Interpolation**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Bao_Depth-Aware_Video_Frame_Interpolation_CVPR_2019_paper.pdf)
- [**Towards Accurate Generative Models of Video: A New Metric & Challenges**](https://arxiv.org/abs/1812.01717.pdf)
- [**Deep Video Frame Interpolation using Cyclic Frame Generation**](https://www.citi.sinica.edu.tw/papers/yylin/6497-F.pdf)
- [**[1907.13622] Video Stitching for Linear Camera Arrays**](https://arxiv.org/abs/1907.13622)
- [**Deformable GANs for Pose-based Human Image Generation**](https://arxiv.org/abs/1801.00055.pdf)
- [**U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation**](https://arxiv.org/abs/1907.10830v1.pdf)
- [**Pose Guided Person Image Generation**](https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf)
- [**High-Fidelity Image Generation With Fewer Labels**](https://arxiv.org/abs/1903.02271.pdf)
- [**[1906.04728] Shapes and Context: In-the-Wild Image Synthesis & Manipulation**](https://arxiv.org/abs/1906.04728)
- [**AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks**](https://arxiv.org/abs/1711.10485.pdf)
- [**Photographic Text-to-Image Synthesis With a Hierarchically-Nested Adversarial Network**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Photographic_Text-to-Image_Synthesis_CVPR_2018_paper.pdf)
- [**Modular Generative Adversarial Networks**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bo_Zhao_Modular_Generative_Adversarial_ECCV_2018_paper.pdf)
- [**Learning Character-Agnostic Motion for Motion Retargeting in 2D**](https://arxiv.org/abs/1905.01680.pdf)
- [**Learning 3D Human Dynamics From Video**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kanazawa_Learning_3D_Human_Dynamics_From_Video_CVPR_2019_paper.pdf)
- [**[NeurIPS 2018] How to Start Training: The Effect of Initialization and Architecture - YouTube**](https://www.youtube.com/watch?v=F-85-1lU6DM)
- [**Inserting Videos Into Videos**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Inserting_Videos_Into_Videos_CVPR_2019_paper.pdf)
- [**Multimodal Unsupervised Image-to-image Translation**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper.pdf)
- [**A Style-Based Generator Architecture for Generative Adversarial Networks**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf)
- [**Diverse Image-to-Image Translation via Disentangled Representations**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Hsin-Ying_Lee_Diverse_Image-to-Image_Translation_ECCV_2018_paper.pdf)
- [**Densely Connected Pyramid Dehazing Network**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Densely_Connected_Pyramid_CVPR_2018_paper.pdf)
- [**StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf)
- [**Self-Attention Generative Adversarial Networks**](https://arxiv.org/abs/1805.08318.pdf)
- [**Glow: Generative Flow with Invertible 1x1 Convolutions**](http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions)
- [**Residual Dense Network for Image Super-Resolution**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf)
- [**Generative Image Inpainting With Contextual Attention**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Generative_Image_Inpainting_CVPR_2018_paper.pdf)
- [**10.1109/TPAMI.2018.2856256**](http://sci-hub.tw/https://ieeexplore.ieee.org/abstract/document/8411144 | Sci-Hub | StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1 )
- [**Auto-painter: Cartoon Image Generation from Sketch by Using Conditional Generative Adversarial Networks**](https://arxiv.org/abs/1705.01908.pdf)
- [**Imagine This! Scripts to Compositions to Videos**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Tanmay_Gupta_Imagine_This_Scripts_ECCV_2018_paper.pdf)
- [**Automatic manga colorization with color style by generative adversarial nets - IEEE Conference Publication**](https://ieeexplore.ieee.org/abstract/document/8022768)
- [**Neural scene representation and rendering**](http://sci-hub.tw/https://science.sciencemag.org/content/360/6394/1204.abstract)
- [**Which Training Methods for GANs do actually Converge?**](https://arxiv.org/abs/1801.04406.pdf)
- [**Deep Video Inpainting**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Deep_Video_Inpainting_CVPR_2019_paper.pdf)
- [**PacGAN: The power of two samples in generative adversarial networks**](http://papers.nips.cc/paper/7423-pacgan-the-power-of-two-samples-in-generative-adversarial-networks.pdf)
- [**FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models**](https://arxiv.org/abs/1810.01367.pdf)
- [**User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks**](https://arxiv.org/abs/1808.03240.pdf)
- [**Generative Adversarial Text to Image Synthesis**](http://proceedings.mlr.press/v48/reed16.pdf)
- [**Stacked_Generative_Adversarial Networks**](http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.html)
- [**Neural Photo Editing with Introspective Adversarial Networks**](https://arxiv.org/abs/1609.07093.pdf)
- [**LumièreNet: Lecture Video Synthesis from Audio**](https://arxiv.org/abs/1907.02253v1.pdf)
- [**pix2pixHD_ntire2018**](http://www.vision.ee.ethz.ch/ntire18/talks/Ming-YuLiu_pix2pixHD_NTIRE2018talk.pdf)
