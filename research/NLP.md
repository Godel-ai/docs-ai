# NLP

---

https://arxiv.org/abs/2005.00571v1 | [2005.00571v1] Learning Collaborative Agents with Rule Guidance for Knowledge Graph Reasoning
https://arxiv.org/abs/2005.00782v1 | [2005.00782v1] Can BERT Reason? Logically Equivalent Probes for Evaluating the Inference Capabilities of Language Models
https://paperswithcode.com/paper/common-knowledge-concept-recognition-for-seva | Common-Knowledge Concept Recognition for SEVA |
https://arxiv.org/abs/2003.04881v2.pdf | Neural Networks are Surprisingly Modular
https://arxiv.org/abs/1912.13283v1 | [1912.13283v1] oLMpics -- On what Language Model Pre-training Captures
https://github.com/shenweichen/DeepCTR | shenweichen/DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models.

## Extractive Summarization of Scientific Documents

https://arxiv.org/abs/2002.10640.pdf | Differentiable Reasoning over a Virtual Knowledge Base
https://arxiv.org/abs/1912.10824 | [1912.10824] Differentiable Reasoning on Large Knowledge Bases and Natural Language
https://github.com/uclnlp/gntp | uclnlp/gntp

---

### Summarization

https://paperswithcode.com/paper/graph-structured-referring-expression | - Graph-Structured Referring Expression Reasoning in The Wild
https://github.com/mesnico/TERN | mesnico/TERN: Code and Resources for the Transformer Encoder Reasoning Network (TERN) - https://arxiv.org/abs/2004.09144
https://github.com/maszhongming/MatchSum | maszhongming/MatchSum: Code for ACL 2020 paper: "Extractive Summarization as Text Matching"
https://arxiv.org/abs/2004.08795 | [2004.08795] Extractive Summarization as Text Matching
https://paperswithcode.com/task/automatic-writing | - Automatic Writing
https://paperswithcode.com/task/extractive-document-summarization/codeless#code | - Extractive Document Summarization
https://paperswithcode.com/task/reader-aware-summarization | - Reader-Aware Summarization
https://paperswithcode.com/paper/text-summarization-with-pretrained-encoders | - Text Summarization with Pretrained Encoders
https://paperswithcode.com/paper/at-which-level-should-we-extract-an-empirical | - At Which Level Should We Extract? An Empirical Study on Extractive Document Summarization
https://paperswithcode.com/paper/language-model-pre-training-for-hierarchical | - Language Model Pre-training for Hierarchical Document Representations
https://paperswithcode.com/paper/neural-latent-extractive-document-1 | - Neural Latent Extractive Document Summarization
https://github.com/the-Quert/iNLPfun | the-Quert/inlpfun: Tensorflow code , pre-trained models, and papers of Natural Language Processing
https://scholar.google.com/scholar?safe=active&sxsrf=ALeKk02hxnKrtc4YY-qeMhGArujSOR-D8Q:1587720646144&gs_lcp=CgZwc3ktYWIQAzoECAAQRzoCCAA6BggAEBYQHjoFCCEQoAE6BwghEAoQoAE6BAghEBU6CAghEBYQHRAeUKI0WPe2AWDMuAFoBHACeACAAbsBiAHDJpIBBDAuMzOYAQCgAQGqAQdnd3Mtd2l6&uact=5&um=1&ie=UTF-8&lr&cites=3309430449122454281 | Liu: Graph summarization methods and applications: A survey - Google Scholar
https://scholar.google.co.in/scholar?q=prerequisite+graph+generation+for+summarization&hl=en&as_sdt=0&as_vis=1&oi=scholart | prerequisite graph generation for summarization - Google Scholar
https://www.google.com/search?q=Tutorialbank%3A+A+manually-collected+corpus+for+prerequisite+chains%2C+survey+extraction+and+resource+recommendation&oq=Tutorialbank%3A+A+manually-collected+corpus+for+prerequisite+chains%2C+survey+extraction+and+resource+recommendation&aqs=chrome..69i57j69i58j69i60&sourceid=chrome&ie=UTF-8 | Tutorialbank: A manually-collected corpus for prerequisite chains, survey extraction and resource recommendation - Google Search
https://arxiv.org/abs/1805.04617.pdf | TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation
http://aan.how/about/ | AAN: About
https://scholar.google.com/scholar?as_ylo=2019&hl=en&as_sdt=0,5&sciodt=0,5&cites=1289960751808453189&scipsc= | Abu-Jbara: Coherent citation-based summarization... - Google Scholar
http://archiv.ub.uni-heidelberg.de/volltextserver/27924/1/Daraksha_Thesis.pdf | Daraksha_Thesis.pdf
https://arxiv.org/abs/1906.01351 | [1906.01351] TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks
https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11041/110410U/Automatic-paper-summary-generation-from-visual-and-textual-information/10.1117/12.2522789.short?SSO=1 | Automatic paper summary generation from visual and textual information
https://arxiv.org/abs/1909.01716.pdf | ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks
https://arxiv.org/abs/2004.02843 | [2004.02843] Improved Code Summarization via a Graph Neural Network
https://www.aclweb.org/anthology/C10-1039.pdf | Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant Opinions
https://tsafavi.github.io/assets/pdf/liu-2018-graph-summarization.pdf | CSUR5103-62
http://cs-people.bu.edu/evimaria/papers/Social-net.pdf | Social-net.pdf
https://github.com/google-research/electra | google-research/electra: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators

---

### Storytelling

https://arxiv.org/abs/1902.01109.pdf | Strategies for Structuring Story Generation
https://www.aclweb.org/anthology/W19-3405.pdf | Guided Neural Language Generation for Automated Storytelling
https://www.google.com/search?q=Guided+Neural+Language+Generation+for+Automated+Storytelling&oq=Guided+Neural+Language+Generation+for+Automated+Storytelling&aqs=chrome..69i57j69i59j69i61l3&sourceid=chrome&ie=UTF-8 | Guided Neural Language Generation for Automated Storytelling - Google Search
https://paperswithcode.com/search?q_meta=&q=story+generation | - Papers With Code : Search for story generation
https://paperswithcode.com/paper/a-knowledge-enhanced-pretraining-model-for | - A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation
https://github.com/thu-coai/CommonsenseStoryGen | thu-coai/CommonsenseStoryGen: Implementation for paper "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation"

---

https://arxiv.org/abs/2004.10151v1.pdf | Experience Grounds Language
https://arxiv.org/abs/2004.07780v1.pdf | Shortcut Learning in Deep Neural Networks
https://arxiv.org/abs/1908.05646.pdf | SenseBERT: Driving Some Sense into BERT

### NLP

https://arxiv.org/abs/2005.07647.pdf | Finding Experts in Transformer Models
https://arxiv.org/abs/2005.07648.pdf | Grounding Language in Play
https://arxiv.org/abs/2004.14373.pdf | ToTTo: A Controlled Table-To-Text Generation Dataset
https://homes.cs.washington.edu/~msap/pdfs/sap2020recollectionImagination.pdf | Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models
https://arxiv.org/abs/1911.12753v1.pdf |Inducing Relational Knowledge from BERT
https://arxiv.org/abs/1906.01603.pdf | Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study
https://arxiv.org/abs/2004.15011v1.pdf | TLDR: Extreme Summarization of Scientific Documents
https://distill.pub/2019/gan-open-problems/ | Open Questions about Generative Adversarial Networks
https://github.com/allenai/scitldr | allenai/scitldr

---

#### Chatbots

https://arxiv.org/abs/1804.08217.pdf | Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems
https://www.aclweb.org/anthology/W16-6603.pdf | Generating English from Abstract Meaning Representations
https://arxiv.org/abs/1801.07736 | [1801.07736] MaskGAN: Better Text Generation via Filling in the**\_\_**
https://arxiv.org/abs/1909.03186.pdf |On Extractive and Abstractive Neural Document Summarization with Transformer Language Models
https://arxiv.org/abs/1709.02349.pdf | A Deep Reinforcement Learning Chatbot

### Knowledge graph

https://www.aclweb.org/anthology/D19-6607.pdf | Scalable Knowledge Graph Construction from Text Collections
https://arxiv.org/abs/1912.02164v2.pdf | Plug and Play Language Models: A Simple Approach to Controlled Text Generation

---

### Random

https://scholar.google.com/scholar?safe=active&rlz=1C1CHBF_enUS858US858&sxsrf=ACYBGNSAPKh-P1RHf4kuZ0Ciw-tc4coy7A:1578892703748&uact=5&um=1&ie=UTF-8&lr&cites=10392085391906571188 | Katti: Chargrid: Towards understanding 2d documents - Google Scholar
http://proceedings.mlr.press/v97/chu19b.html | MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
https://github.com/sosuperic/MeanSum | sosuperic/MeanSum
https://arxiv.org/abs/1906.01749.pdf | 1906.01749.pdf
https://arxiv.org/abs/1910.11411.pdf | 1910.11411.pdf
https://www.aclweb.org/anthology/D18-1476.pdf | Chargrid: Towards Understanding 2D Documents
https://arxiv.org/abs/1909.03186.pdf | AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide
https://arxiv.org/abs/1706.06681.pdf | 1706.06681.pdf
https://scholar.google.com/scholar?start=10&hl=en&as_sdt=2005&sciodt=0,5&cites=6702357170880053318&scipsc= | Gehrmann: Bottom-up abstractive summarization - Google Scholar
https://www.sciencedirect.com/science/article/pii/S0957417418307735 | Abstractive summarization: An overview of the state of the art - ScienceDirect
https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf | Language Models are Unsupervised Multitask Learners
https://arxiv.org/abs/1908.08345 | [1908.08345] Text Summarization with Pretrained Encoders
https://arxiv.org/abs/1908.08960 | [1908.08960] Neural Text Summarization: A Critical Evaluation
https://arxiv.org/abs/1910.08435.pdf | 1910.08435.pdf
https://paperswithcode.com/task/dialogue-generation | : Dialogue Generation
https://paperswithcode.com/sota | Browse the State-of-the-Art in Machine Learning
https://paperswithcode.com/task/knowledge-base-population/latest#code | : Knowledge Base Population
https://paperswithcode.com/task/recommendation-systems/latest#code | : Recommendation Systems
https://paperswithcode.com/task/neural-network-compression/latest#code | : Neural Network Compression
https://paperswithcode.com/task/knowledge-graphs/latest#code | : Knowledge Graphs
https://paperswithcode.com/task/knowledge-base-completion | : Knowledge Base Completion
https://paperswithcode.com/task/extractive-document-summarization/latest#code | : Extractive Document Summarization
https://paperswithcode.com/task/abstractive-text-summarization/latest#code | : Abstractive Text Summarization
https://paperswithcode.com/task/document-summarization | : Document Summarization
https://paperswithcode.com/task/relation-extraction/latest#code | : Relation Extraction
https://paperswithcode.com/task/dialogue-generation/latest#code | : Dialogue Generation
https://arxiv.org/abs/2004.03844v1 | [2004.03844v1] Poor Man's BERT: Smaller and Faster Transformer Models
https://language-play.github.io/ | https://language-play.github.io

https://scholar.google.co.in/scholar?cites=10202908905437352262&as_sdt=2005&sciodt=0,5&hl=en | Yasunaga: Graph-based neural multi-document summarization - Google Scholar
https://paperswithcode.com/task/multi-document-summarization | : Multi-Document Summarization
https://arxiv.org/abs/1910.11411 | [1910.11411] Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations
https://arxiv.org/abs/1906.01749 | [1906.01749] Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model
http://proceedings.mlr.press/v97/chu19b.html | MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
https://arxiv.org/abs/1910.08435 | [1910.08435] Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs
https://arxiv.org/abs/1706.06681 | [1706.06681] Graph-based Neural Multi-Document Summarization
https://scholar.google.co.in/scholar?cites=748046702805969208&as_sdt=2005&sciodt=0,5&hl=en | Li: Deep recurrent generative decoder for abstractive... - Google Scholar
https://arxiv.org/abs/1909.13717 | [1909.13717] Retrieval-based Goal-Oriented Dialogue Generation
https://arxiv.org/abs/1909.03186.pdf | On Extractive and Abstractive Neural Document Summarization with Transformer Language Models
https://scholar.google.com/scholar?safe=active&rlz=1C1CHBF_enUS858US858&sxsrf=ACYBGNSAPKh-P1RHf4kuZ0Ciw-tc4coy7A:1578892703748&uact=5&um=1&ie=UTF-8&lr&cites=10392085391906571188 | Katti: Chargrid: Towards understanding 2d documents - Google Scholar
https://arxiv.org/abs/1910.03678.pdf | 1910.03678.pdf
https://www.aclweb.org/anthology/D18-1476.pdf | Chargrid: Towards Understanding 2D Documents
https://arxiv.org/abs/1806.00069 | [1806.00069] Explaining Explanations: An Overview of Interpretability of Machine Learning

https://paperswithcode.com/task/knowledge-base-population | : Knowledge Base Population
https://paperswithcode.com/paper/position-aware-attention-and-supervised-data | : Position-aware Attention and Supervised Data Improve Slot Filling
https://github.com/Bread-and-Code/Text-Summarization | Bread-and-Code/Text-Summarization: A text document will be provided and it'll produce it's summary
https://www.aclweb.org/anthology/D19-6607.pdf | Scalable Knowledge Graph Construction from Text Collections
https://medium.com/@mgalkin/knowledge-graphs-nlp-emnlp-2019-part-i-e4e69fd7957c#e17a | Knowledge Graphs & NLP @ EMNLP 2019 Part I - Michael Galkin - Medium
https://arxiv.org/abs/1910.08435.pdf | 1910.08435.pdf
https://github.com/dstlry/dstlr/commits/master | Commits Â· dstlry/dstlr
https://dstlry.github.io/ | What is dstlr? | dstlr

https://github.com/TensorSpeech/TensorflowTTS | TensorSpeech/TensorflowTTS: TensorflowTTS: Real-Time State-of-the-art Speech Synthesis for Tensorflow 2
https://github.com/TensorSpeech/TensorflowTTS | TensorSpeech/TensorflowTTS: TensorflowTTS: Real-Time State-of-the-art Speech Synthesis for Tensorflow 2

https://scholar.google.com/scholar?rlz=1C1CHBF_enUS858US858&um=1&ie=UTF-8&lr&cites=3530396919946840784 | Wu: Transferable Multi-Domain State Generator for... - Google Scholar
https://arxiv.org/abs/1907.01669 | [1907.01669] MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines
https://arxiv.org/abs/1908.01946 | [1908.01946] Dialog State Tracking: A Neural Reading Comprehension Approach
https://arxiv.org/abs/1905.08743.pdf | 1905.08743.pdf
https://github.com/jasonwu0731/trade-dst | jasonwu0731/trade-dst: Source code for transferable dialogue state generator (TRADE, Wu et al., 2019). https://arxiv.org/abs/1905.08743
https://arxiv.org/abs/1906.02361 | [1906.02361] Explain Yourself! Leveraging Language Models for Commonsense Reasoning
https://arxiv.org/abs/1808.01400 | [1808.01400] code2seq: Generating Sequences from Structured Representations of Code
https://arxiv.org/abs/1811.12359 | [1811.12359] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
https://github.com/facebookresearch/LAMA | facebookresearch/LAMA: LAnguage Model Analysis
https://arxiv.org/abs/1909.01066v2.pdf | Language Models as Knowledge Bases
